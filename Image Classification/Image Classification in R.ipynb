{"cells":[{"metadata":{},"cell_type":"markdown","source":"Import Keras","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"library(keras)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the Data Generators","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_gen <- image_data_generator(\n      preprocessing_function = nasnet_preprocess_input, #split into 80% training and 20% validation data\n      rotation_range = 40, #Data Augmentation\n      width_shift_range = 0.2,\n      height_shift_range = 0.2,\n      shear_range = 0.2,\n      zoom_range = 0.2,\n      horizontal_flip = TRUE,\n      fill_mode = \"nearest\"\n)\n\n# Validation data shouldn't be augmented! But it should also be scaled.\nvalid_data_gen <- image_data_generator(\n      preprocessing_function = nasnet_preprocess_input\n)\n   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Specify the Params","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width <- 331L\nimg_height <- 331L\nchannels <- 3L #RGB has 3 colours\n\nbatch_size <- 32L\n\nnum_classes <- 3L\n\ntrain_image_files_path <- '../input/testing/Train'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the Data and convert IMG to arrays","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# training images\ntrain_it <- flow_images_from_directory(train_image_files_path, \n                                       train_data_gen,\n                                       target_size = c(img_width, img_height),\n                                       class_mode = \"categorical\",\n                                       batch_size = batch_size,\n                                       shuffle = TRUE) \n\n# validation images\nval_it <- flow_images_from_directory(train_image_files_path, \n                                     valid_data_gen,\n                                     target_size = c(img_width, img_height),\n                                     class_mode = \"categorical\",\n                                     batch_size = batch_size,\n                                     shuffle = FALSE)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check if loading worked","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"str(reticulate::iter_next(train_it))\nstr(reticulate::iter_next(val_it))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load the pretrained Model (Transfer Learning)\nbase_model <- application_nasnetlarge(weights = 'imagenet', include_top = FALSE, \n                                input_shape = c(img_width, img_height, channels)) # We stripped off the input layer and the classifier layer\n\n# Examine the Architecture\nsummary(base_model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Fine Tuning","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# define new classifier layers\npredictions <- base_model$output %>%\n  layer_global_average_pooling_2d(trainable = T) %>%\n  layer_dense(num_classes, trainable=T) %>%   \n  layer_activation(\"softmax\", trainable=T) # Softmax is used for Multi-label classification\n\n# add the layers to the model\nmodel <- keras_model(inputs = base_model$input, outputs = predictions)\nfreeze_weights(base_model) # Freeze the weights of the base_model as we only want to train the classifier\n\nsummary(model)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Compile the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model %>% compile(\n  optimizer = optimizer_sgd(lr = 0.001, momentum = 0.9, nesterov = TRUE),\n  loss = 'categorical_crossentropy', # used in multi-label classification\n  metrics = c('accuracy')\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Start Training","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"hist <- model %>% fit_generator(\n  train_it,\n  validation_data = val_it,\n  epochs=10, \n  steps_per_epoch = 1,\n  validation_steps = 1,\n  verbose = 2\n)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save the Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"save_model_hdf5(model, \"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load Model","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"model <- load_model_hdf5(\"model.h5\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualise the Training History","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"plot(hist)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"z <- evaluate_generator(model, \n                        val_it, \n                        steps = 1)\nz","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Load the Test Image","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# load the image\nimg_path <- \"../input/testing/Train/1/download1.jpeg\"\nimg <- image_load(img_path, target_size = c(img_width,img_height))\nx <- image_to_array(img)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Preprocess the IMG","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# ensure we have a 4d tensor with single element in the batch dimension,\n# the preprocess the input for prediction using resnet50\nx <- array_reshape(x, c(1, dim(x)))\nx <- nasnet_preprocess_input(x)\nx","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Prediction","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# make predictions then decode and print them\npred <- model %>% predict(x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Implement a dictionary (Key-Value) for mapping prediction to labels\n\nd <- list()\n\nd[[1]] <- \"Apples\"\n\nd[[2]] <- \"Oranges\"\n\nd[[3]] <- \"Bananas\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"d[[which.max(pred)]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test_image_files_path <- '../input/testing-2/Test'\n\n# test_data_gen <- image_data_generator(\n#       preprocessing_function = nasnet_preprocess_input\n# ) \n\n\n# test_it <- flow_images_from_directory(test_image_files_path, \n#                                      test_data_gen,\n#                                      target_size = c(img_width, img_height),\n#                                      class_mode = NULL,\n#                                      batch_size = batch_size,\n#                                      shuffle = FALSE)\n\n# str(reticulate::iter_next(test_it))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"TESTING","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#' In this example we fine tune Mobile Net to better predict cats and\n#' dogs in photos. It also demonstrates the usage of image data generators\n#' for efficient preprocessing and training.\n#' \n#' It's preferable to run this example in a GPU.\n\n# Download data -----------------------------------------------------------\n\ndownload.file(\n  \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\", \n  destfile = \"cats-dogs.zip\"\n)\n\n# Pre-processing ----------------------------------------------------------\n\nzip::unzip(\"cats-dogs.zip\", exdir = \"data-raw\")\n\n# We will organize images in the following structure:\n# data/\n#     train/\n#          Cat/\n#          Dog/\n#     validation\n#          Cat/\n#          Dog/\n#     test/\n#          images/\n#\n\nall_imgs <- fs::dir_ls(\n  \"data-raw/PetImages/\", \n  recursive = TRUE, \n  type = \"file\",\n  glob = \"*.jpg\"\n)\n\n# some images are corrupt and we exclude them\n# this will make sure all images can be read.\nfor (im in all_imgs) {\n  out <- try(magick::image_read(im), silent = TRUE)\n  if (inherits(out, \"try-error\")) {\n    fs::file_delete(im)\n    message(\"removed image: \", im)\n  }\n}\n\n# re-list all imgs\nall_imgs <- fs::dir_ls(\n  \"data-raw/PetImages/\", \n  recursive = TRUE, \n  type = \"file\",\n  glob = \"*.jpg\"\n)\n\nset.seed(5)\n\ntraining_imgs <- sample(all_imgs, size = length(all_imgs)/2)\nvalidation_imgs <- sample(all_imgs[!all_imgs %in% training_imgs], size = length(all_imgs)/4)         \ntesting_imgs <- all_imgs[!all_imgs %in% c(training_imgs, validation_imgs)]\n\n# create directory structure\nfs::dir_create(c(\n  \"data/train/Cat\",\n  \"data/train/Dog\",\n  \"data/validation/Cat\",\n  \"data/validation/Dog\",\n  \"data/test/images\"\n))\n\n# copy training images\nfs::file_copy(\n  path = training_imgs, \n  new_path = gsub(\"data-raw/PetImages\", \"data/train\", training_imgs)\n)\n\n# copy valid images\nfs::file_copy(\n  path = validation_imgs, \n  new_path = gsub(\"data-raw/PetImages\", \"data/validation\", validation_imgs)\n)\n\n# copy testing imgs\nfs::file_copy(\n  path = testing_imgs,\n  new_path = gsub(\"data-raw/PetImages/(Dog|Cat)/\", \"data/test/images/\\\\1\", testing_imgs)\n)\n\n# Image flow --------------------------------------------------------------\n\nlibrary(keras)\n\ntraining_image_gen <- image_data_generator(\n  rotation_range = 20,\n  width_shift_range = 0.2,\n  height_shift_range = 0.2,\n  horizontal_flip = TRUE,\n  preprocessing_function = imagenet_preprocess_input\n)\n\nvalidation_image_gen <- image_data_generator(\n  preprocessing_function = imagenet_preprocess_input\n)\n\ntraining_image_flow <- flow_images_from_directory(\n  directory = \"data/train/\", \n  generator = training_image_gen, \n  class_mode = \"binary\",\n  batch_size = 100,\n  target_size = c(224, 224), \n)\n\nvalidation_image_flow <- flow_images_from_directory(\n  directory = \"data/validation/\", \n  generator = validation_image_gen, \n  class_mode = \"binary\",\n  batch_size = 100,\n  target_size = c(224, 224), \n  shuffle = FALSE\n)\n\n# Model -------------------------------------------------------------------\n\nmob <- application_mobilenet(include_top = FALSE, pooling = \"avg\")\nfreeze_weights(mob)\n\nmodel <- keras_model_sequential() %>% \n  mob() %>% \n  layer_dense(256, activation = \"relu\") %>% \n  layer_dropout(rate = 0.2) %>% \n  layer_dense(units = 1, activation = \"sigmoid\")\n\nmodel %>% \n  compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit_generator(\n  generator = training_image_flow, \n  epochs = 1, \n  steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,\n  validation_data = validation_image_flow,\n  validation_steps = validation_image_flow$n/validation_image_flow$batch_size\n)\n\n# now top layers weights are fine, we can unfreeze the lower layer weights.\nunfreeze_weights(mob)\n\nmodel %>% \n  compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = \"accuracy\")\n\nmodel %>% fit_generator(\n  generator = training_image_flow, \n  epochs = 3, \n  steps_per_epoch = training_image_flow$n/training_image_flow$batch_size,\n  validation_data = validation_image_flow,\n  validation_steps = validation_image_flow$n/validation_image_flow$batch_size\n)\n\n# Generate predictions for test data --------------------------------------\n\ntest_flow <- flow_images_from_directory(\n  generator = validation_image_gen,\n  directory = \"data/test\", \n  target_size = c(224, 224),\n  class_mode = NULL,\n  shuffle = FALSE\n)\n\npredictions <- predict_generator(\n  model, \n  test_flow,\n  steps = test_flow$n/test_flow$batch_size\n)\n\nmagick::image_read(testing_imgs[1])\npredictions[1]\n\nmagick::image_read(testing_imgs[6250])\npredictions[6250]\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"3.6.3"}},"nbformat":4,"nbformat_minor":4}