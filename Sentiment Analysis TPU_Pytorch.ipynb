{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TPU_(DistilBert 4vs5).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPeU9AM6tflD",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "eeacee2f-ee1a-4a97-fb8a-98ec95e88b59"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoffxBbHWf5q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "outputId": "0b5a081f-5fcc-4e8f-d422-33beb9a73237"
      },
      "source": [
        "VERSION = \"1.5\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n",
        "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
        "!python pytorch-xla-env-setup.py --version $VERSION"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  5115  100  5115    0     0  26640      0 --:--:-- --:--:-- --:--:-- 26640\n",
            "Updating... This may take around 2 minutes.\n",
            "Updating TPU runtime to pytorch-1.5 ...\n",
            "Uninstalling torch-1.5.0a0+ab660ae:\n",
            "  Successfully uninstalled torch-1.5.0a0+ab660ae\n",
            "Uninstalling torchvision-0.6.0a0+3c254fb:\n",
            "  Successfully uninstalled torchvision-0.6.0a0+3c254fb\n",
            "Copying gs://tpu-pytorch/wheels/torch-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][ 79.0 MiB/ 79.0 MiB]                                                \n",
            "Operation completed over 1 objects/79.0 MiB.                                     \n",
            "Copying gs://tpu-pytorch/wheels/torch_xla-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "- [1 files][106.6 MiB/106.6 MiB]                                                \n",
            "Operation completed over 1 objects/106.6 MiB.                                    \n",
            "Copying gs://tpu-pytorch/wheels/torchvision-1.5-cp36-cp36m-linux_x86_64.whl...\n",
            "/ [1 files][  2.5 MiB/  2.5 MiB]                                                \n",
            "Operation completed over 1 objects/2.5 MiB.                                      \n",
            "Processing ./torch-1.5-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==1.5) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.5) (0.16.0)\n",
            "Done updating TPU runtime\n",
            "\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n",
            "Installing collected packages: torch\n",
            "Successfully installed torch-1.5.0a0+ab660ae\n",
            "Requirement already satisfied: torch-xla==1.5 from file:///content/torch_xla-1.5-cp36-cp36m-linux_x86_64.whl in /usr/local/lib/python3.6/dist-packages (1.5)\n",
            "Processing ./torchvision-1.5-cp36-cp36m-linux_x86_64.whl\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (7.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.18.5)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==1.5) (1.5.0a0+ab660ae)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==1.5) (0.16.0)\n",
            "Installing collected packages: torchvision\n",
            "Successfully installed torchvision-0.6.0a0+3c254fb\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "libomp5 is already the newest version (5.0.1-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmbb1w73Wjwz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "e5394c96-a59b-4222-9f40-115586190107"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pekdnM7NWlvC",
        "colab_type": "text"
      },
      "source": [
        "Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2_4W7RhWlTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as datautils\n",
        "\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.xla_multiprocessing as xmp\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, AdamW, get_cosine_schedule_with_warmup\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm \n",
        "from sklearn.metrics import roc_auc_score\n",
        "import time, os"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQLiRFQZWte6",
        "colab_type": "text"
      },
      "source": [
        "Data Preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyWXuMT9WqIQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pretrained = 'distilbert-base-uncased'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHf5u4fKkCfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=270):\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts,\n",
        "        add_special_tokens=True, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        truncation=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(pretrained)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0l5S7prV32A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "f1021dfa-e570-415f-a05b-c0c6c2293456"
      },
      "source": [
        "# Read dataset\n",
        "df = pd.read_csv('/content/drive/My Drive/Sentiment Analysis/Clean/train2.csv')\n",
        "df"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>thanks smiling face with smiling eyes</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>i ordered anf black army green color but this ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>good quality size fits</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>excellent product quality</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1 star break because i broke mine shattered my...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146737</th>\n",
              "      <td>but beautiful na lng sa stigma i do not know f...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146738</th>\n",
              "      <td>awesome speed of the ship awesome awesome qual...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146739</th>\n",
              "      <td>product quality is very good recomended produc...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146740</th>\n",
              "      <td>fast seller the child was da play the price wa...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146741</th>\n",
              "      <td>packingnya really neat and also thanks yah cepet</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146742 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   review  label\n",
              "0                   thanks smiling face with smiling eyes      2\n",
              "1       i ordered anf black army green color but this ...      0\n",
              "2                                  good quality size fits      3\n",
              "3                               excellent product quality      4\n",
              "4       1 star break because i broke mine shattered my...      2\n",
              "...                                                   ...    ...\n",
              "146737  but beautiful na lng sa stigma i do not know f...      2\n",
              "146738  awesome speed of the ship awesome awesome qual...      3\n",
              "146739  product quality is very good recomended produc...      4\n",
              "146740  fast seller the child was da play the price wa...      3\n",
              "146741   packingnya really neat and also thanks yah cepet      3\n",
              "\n",
              "[146742 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA3GNNRPWzv3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3daed05-a66d-42a1-fc2e-434432ca596f"
      },
      "source": [
        " # Tokenize the dataset\n",
        "s = time.time()\n",
        "text = regular_encode(list(df['review'].astype(str)), tokenizer)\n",
        "labels = df['label'].values # Labels encoded (see data cleaning notebook)\n",
        "print(\"Elapsed: {:.2f}s\".format(time.time() - s))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Elapsed: 26.55s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vbZ1YKDoYiz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "8cd518d6-5dd0-47ff-991d-9d44ebeb0fad"
      },
      "source": [
        "text"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  101, 15198, 21905, ...,     0,     0,     0],\n",
              "       [  101, 50337, 21535, ...,     0,     0,     0],\n",
              "       [  101, 56237, 41939, ...,     0,     0,     0],\n",
              "       ...,\n",
              "       [  101, 21535, 21905, ...,     0,     0,     0],\n",
              "       [  101, 15040, 22154, ...,     0,     0,     0],\n",
              "       [  101, 78978, 10230, ...,     0,     0,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWlRM9dmyKhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into training and testing\n",
        "tr_sz = int(len(text) * 0.8) #80% | 20%\n",
        "X_train, y_train = torch.tensor(text[:tr_sz]), torch.tensor(labels[:tr_sz], dtype=torch.long)\n",
        "X_valid, y_valid = torch.tensor(text[tr_sz:]), torch.tensor(labels[tr_sz:], dtype=torch.long)\n",
        "\n",
        "# Produce datasets\n",
        "train_set = datautils.TensorDataset(X_train, y_train)\n",
        "valid_set = datautils.TensorDataset(X_valid, y_valid)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmpgDkWtXCAX",
        "colab_type": "text"
      },
      "source": [
        "Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7i4eEBtiWXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def acc_score(preds, actuals):\n",
        "    return (accuracy_score(actuals, preds))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMx5ivJfV-xv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def compute_metrics(preds, actuals):\n",
        "    return (classification_report(actuals, preds, zero_division=0))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUARV6_DG1eR",
        "colab_type": "text"
      },
      "source": [
        "Setting the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd-raUPoGvjv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set flags\n",
        "flags = {\n",
        "    'batch_size': 16,\n",
        "    'num_workers': 8, #number of cores \n",
        "    'num_epochs': 1,\n",
        "    'seed': 42,\n",
        "    'num_labels': 5, #Important\n",
        "    'pretrained': pretrained,\n",
        "    'savedir': '/content/drive/My Drive/Sentiment Analysis/TPU_Checkpoints',\n",
        "    'modelpath': 'model.bin',\n",
        "    'learning_rate': 1e-5,\n",
        "    'weight_decay' : 0.01,\n",
        "    'print_every': 150\n",
        "}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7akjCwEhrFMB",
        "colab_type": "text"
      },
      "source": [
        "Configure the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ce9SIkV8rGVo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "ca04130c-85bc-4581-8eeb-2820e1cfb9d1"
      },
      "source": [
        "    # Configure the model\n",
        "    config = AutoConfig.from_pretrained(flags['pretrained'], num_labels=flags['num_labels'])\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(flags['pretrained'], config=config)\n",
        "    #model.load_state_dict(torch.load(flags['savedir'] + '/' + flags['modelpath']))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-multilingual-cased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jKec3AwXspb",
        "colab_type": "text"
      },
      "source": [
        "Finetuning \n",
        "\n",
        "*   Define the model outside to reduce on memory usage\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpuc8APiXtj8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "    # Set the seed and obtain an XLA device\n",
        "    torch.manual_seed(flags['seed'])\n",
        "    device = xm.xla_device()\n",
        "    print(\"Process\", index, \"obtained, using device:\", xm.xla_real_devices([str(device)])[0]) \n",
        "\n",
        "    # Produce distributed samplers\n",
        "    train_sampler = datautils.distributed.DistributedSampler(\n",
        "        train_set, \n",
        "        num_replicas=xm.xrt_world_size(), \n",
        "        rank=xm.get_ordinal(), \n",
        "        shuffle=True\n",
        "    )\n",
        "    valid_sampler = datautils.distributed.DistributedSampler(\n",
        "        valid_set, \n",
        "        num_replicas=xm.xrt_world_size(), \n",
        "        rank=xm.get_ordinal(), \n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # Create dataloaders\n",
        "    train_loader = datautils.DataLoader(\n",
        "        train_set,\n",
        "        batch_size=flags['batch_size'], \n",
        "        sampler=train_sampler, \n",
        "        num_workers=flags['num_workers'],\n",
        "        drop_last=True\n",
        "    )\n",
        "    valid_loader = datautils.DataLoader(\n",
        "        valid_set,\n",
        "        batch_size=flags['batch_size'], \n",
        "        sampler=valid_sampler, \n",
        "        num_workers=flags['num_workers'],\n",
        "        drop_last=True,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    # This ensures that the pretrained weights will only be\n",
        "    # downloaded once (c/o the master process). It also makes\n",
        "    # sure that the other processes don't attempt to load the\n",
        "    # weights when downloading isn't finished yet.\n",
        "    if not xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "\n",
        "    #send the model to the TPU\n",
        "    model.to(device)\n",
        "    \n",
        "    if xm.is_master_ordinal():\n",
        "        xm.rendezvous('download_only_once')\n",
        "\n",
        "    # Initialize loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss() #uses both LogSoftmax + NLLLoss\n",
        "    #optimizer = optim.SGD(model.parameters(), lr=flags['learning_rate'], weight_decay=flags['weight_decay'], momentum=0.9, nesterov=True)\n",
        "    optimizer = AdamW(model.parameters(), lr=flags['learning_rate'], weight_decay=flags['weight_decay'], correct_bias=True)\n",
        "\n",
        "    # Create Scheduler\n",
        "    total_steps = len(train_loader) * flags['num_epochs']\n",
        "    scheduler = get_cosine_schedule_with_warmup(\n",
        "      optimizer,\n",
        "      num_warmup_steps=100,\n",
        "      num_training_steps=total_steps\n",
        "      )\n",
        "\n",
        "    xm.master_print(\"\\nNumber of training batches: {}\".format(len(train_loader)))\n",
        "    xm.master_print(\"Number of evaluation batches: {}\\n\".format(len(valid_loader)))\n",
        "    \n",
        "    for e in range(1, flags['num_epochs'] + 1):\n",
        "        \n",
        "        # Train Model\n",
        "        model.train()\n",
        "        train_start = time.time()\n",
        "\n",
        "        xm.master_print(\"=\" * 27 + \"Epoch {} of {}\".format(e, flags['num_epochs']) + \"=\" * 27)\n",
        "        para_train_loader = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n",
        "        for i, batch in enumerate(para_train_loader):\n",
        "            x, y = batch\n",
        "            out = model(x)[0]\n",
        "            loss = criterion(out, y)\n",
        "\n",
        "            if i % flags['print_every'] == 0:\n",
        "                xm.master_print('[TRAIN] Iteration {:4} | Loss {:.4f} | Time Elapsed {:.2f} seconds'.format(i, loss.item(),time.time() - train_start))\n",
        "            \n",
        "            # Update model\n",
        "            optimizer.zero_grad() #clear gradients\n",
        "            loss.backward() #calculate gradient\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) #prevent gradient explosion\n",
        "            xm.optimizer_step(optimizer) #update weights\n",
        "            scheduler.step()\n",
        "        \n",
        "        xm.master_print('\\nFinished training no.{} epoch in {:.2f} seconds.\\n'.format(e, time.time() - train_start))\n",
        "\n",
        "        # Evaluate Model\n",
        "        model.eval()\n",
        "        valid_start = time.time()\n",
        "        preds, actuals = [], []\n",
        "        \n",
        "        with torch.no_grad(): #deactivate autograd\n",
        "            xm.master_print('=' * 28 + 'Validation' + '=' * 28)\n",
        "            para_valid_loader = pl.ParallelLoader(valid_loader, [device]).per_device_loader(device)\n",
        "            for i, batch in enumerate(para_valid_loader):\n",
        "                x, y = batch\n",
        "                out = model(x)[0]\n",
        "                loss = criterion(out, y)\n",
        "\n",
        "                sm = torch.nn.LogSoftmax(dim=1) #apply logsoftmax on the logits\n",
        "                out = sm(out)\n",
        "\n",
        "                # Keep track of all outputs and gold labels\n",
        "                actuals.extend(y.cpu().numpy().tolist())\n",
        "                preds.extend(out.cpu().detach().numpy().tolist())\n",
        "\n",
        "                if i % flags['print_every'] == 0:\n",
        "                    xm.master_print('[VALID] Iteration {:4} | Loss {:.4f} | Time Elapsed {:.2f} seconds'.format(i, loss.item(),time.time() - train_start))\n",
        "\n",
        "        preds, actuals = np.array(preds), np.array(actuals)\n",
        "        preds = np.argmax(preds, axis=1) #Convert probabilities to Categories\n",
        "\n",
        "        valid_acc = acc_score(preds, actuals) #Accuracy Score\n",
        "        metrics = compute_metrics(preds, actuals) #Classification Report\n",
        "        \n",
        "        xm.master_print('\\nFinished evaluation in {:.2f} seconds. Validation Accuracy: {:.4f}\\n'.format(time.time() - valid_start, valid_acc))\n",
        "        xm.master_print(metrics) #print only one core\n",
        "\n",
        "        # Save the model\n",
        "        xm.save(model.state_dict(), flags['savedir'] + '/' + flags['modelpath'])"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNE4ghoMam5s",
        "colab_type": "text"
      },
      "source": [
        "Training\n",
        "\n",
        "*   Sigkill error means run out of memory (reduce Batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the training process\n",
        "#os.mkdir(flags['savedir']) if flags['savedir'] not in os.listdir('.') else print ('Already exist')\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVx29uoPawtw",
        "colab_type": "text"
      },
      "source": [
        "Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKq7VijAhfGl",
        "colab_type": "text"
      },
      "source": [
        "Loading in Test Set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlGpuM4WawPt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TestDataset(datautils.Dataset):\n",
        "    def __init__(self, text, ids):\n",
        "        self.text = text\n",
        "        self.ids = ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        ix_text = self.text[idx]\n",
        "        ix_id = self.ids[idx]\n",
        "        \n",
        "        return ix_text, ix_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skknR6M6ay52",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/Sentiment Analysis/Clean/test2.csv')\n",
        "text = regular_encode(list(df_test['review'].astype(str)), tokenizer)\n",
        "ids = list(df_test['review_id'])\n",
        "\n",
        "# Produce a test set\n",
        "test_set = TestDataset(text, ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCcRrK8lP5Oh",
        "colab_type": "text"
      },
      "source": [
        "Review_id is needed to index the results since we are going to split them into 8 cores for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kSQeAArhpmP",
        "colab_type": "text"
      },
      "source": [
        "Constructing the Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecUkpzkbL9p2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f054cdde-c470-4e54-8d55-3d953fce3046"
      },
      "source": [
        "# Configure the model and load the checkpoint\n",
        "config = AutoConfig.from_pretrained(flags['pretrained'], num_labels=flags['num_labels'])\n",
        "model = AutoModelForSequenceClassification.from_pretrained(flags['pretrained'], config=config)\n",
        "model.load_state_dict(torch.load(flags['savedir'] + '/' + flags['modelpath']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0vLNx5H1maS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_fn(index, flags):\n",
        "    # Set the seed and obtain an XLA device\n",
        "    torch.manual_seed(flags['seed'])\n",
        "    device = xm.xla_device()\n",
        "    print(\"Process\", index, \"obtained, using device:\", xm.xla_real_devices([str(device)])[0])\n",
        "\n",
        "    # Produce a distributed sampler and a data loader\n",
        "    test_sampler = datautils.distributed.DistributedSampler(\n",
        "        test_set,\n",
        "        num_replicas=xm.xrt_world_size(),\n",
        "        rank=xm.get_ordinal(),\n",
        "        shuffle=False\n",
        "    )\n",
        "    test_loader = datautils.DataLoader(\n",
        "        test_set,\n",
        "        batch_size=flags['batch_size'],\n",
        "        sampler=test_sampler,\n",
        "        pin_memory=False,\n",
        "        drop_last=False,\n",
        "        num_workers=flags['num_workers']\n",
        "    )\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    xm.master_print(\"\\nNumber of testing batches: {}\\n\".format(len(test_loader)))\n",
        "\n",
        "    # Run inferencing\n",
        "    model.eval()\n",
        "    preds, ids = [], []\n",
        "    test_start = time.time()\n",
        "\n",
        "    xm.master_print('=' * 25 + 'Inference' + '=' * 25)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(test_loader):\n",
        "            x, idx = batch\n",
        "            x = x.to(device)\n",
        "            out = model(x)[0]\n",
        "            sm = torch.nn.LogSoftmax(dim=1)\n",
        "            out = sm(out)\n",
        "            preds.extend(out.cpu().detach().numpy().tolist())\n",
        "            ids.append(idx)\n",
        "            if i % flags['print_every'] == 0: \n",
        "                xm.master_print('Inferencing on step {:4} | Time elapsed: {:.2f} seconds'.format(i, time.time() - test_start))\n",
        "        preds = np.array(preds)    \n",
        "\n",
        "    # Save the predictions and associated IDs into a temporary file\n",
        "    with open('{}/preds_{}.pt'.format(flags['savedir'], xm.xla_real_devices([str(device)])[0]), 'wb') as f:\n",
        "        torch.save([ids, preds], f)\n",
        "\n",
        "    xm.master_print('\\nFinished inferencing in {:.2f} seconds.\\n'.format(time.time() - test_start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ovldwKRkBSc",
        "colab_type": "text"
      },
      "source": [
        "Start Prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtjQWlUPkDEA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "a7a15725-ff61-4411-ddb3-8667f11ef09d"
      },
      "source": [
        "# Start the processes\n",
        "xmp.spawn(map_fn, args=(flags,), nprocs=8, start_method='fork') "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Process 0 obtained, using device: TPU:0\n",
            "\n",
            "Number of testing batches: 945\n",
            "\n",
            "=========================Inference=========================\n",
            "Process 7 obtained, using device: TPU:7\n",
            "Process 1 obtained, using device: TPU:1\n",
            "Process 3 obtained, using device: TPU:3\n",
            "Inferencing on step    0 | Time elapsed: 5.10 seconds\n",
            "Process 4 obtained, using device: TPU:4\n",
            "Process 2 obtained, using device: TPU:2\n",
            "Process 5 obtained, using device: TPU:5\n",
            "Process 6 obtained, using device: TPU:6\n",
            "Inferencing on step  150 | Time elapsed: 49.21 seconds\n",
            "Inferencing on step  300 | Time elapsed: 69.42 seconds\n",
            "Inferencing on step  450 | Time elapsed: 90.01 seconds\n",
            "Inferencing on step  600 | Time elapsed: 109.31 seconds\n",
            "Inferencing on step  750 | Time elapsed: 128.91 seconds\n",
            "Inferencing on step  900 | Time elapsed: 148.96 seconds\n",
            "\n",
            "Finished inferencing in 159.70 seconds.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtlaaZMBl5ou",
        "colab_type": "text"
      },
      "source": [
        "Remove Duplicates since processing done in all 8 cores (same batch_size in each core)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csPtztHWEptd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "53c23b81-9c0d-4ebc-db51-12aab5683e37"
      },
      "source": [
        "# Load all prediction files\n",
        "all_ids, all_preds = [], []\n",
        "\n",
        "for i in range(8):\n",
        "    with open('{}/preds_TPU:{}.pt'.format(flags['savedir'], i), 'rb') as f:\n",
        "        idx, preds = torch.load(f)\n",
        "        all_preds.extend(preds)\n",
        "        for id in idx:\n",
        "            all_ids.extend(id.numpy()) #convert tensor to number and to combine in a list\n",
        "preds = np.array(all_preds)\n",
        "preds = np.argmax(preds, axis=1)\n",
        "preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 0, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b2uvBbh8SA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keys = {0:1, 1:2, 2:3, 3:4, 4:5}\n",
        "rating = []\n",
        "\n",
        "for i in preds:\n",
        "  rating.append(keys[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeC3UGNw7ldP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine and remove duplicates\n",
        "df_result = pd.DataFrame(data={'review_id': all_ids})\n",
        "df_result['rating'] = rating\n",
        "df_result.drop_duplicates(keep='first', subset='review_id', inplace=True)\n",
        "#sort in ascending order\n",
        "df_result.sort_values(by=['review_id'], inplace=True)\n",
        "df_result.set_index(['review_id'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save CSV\n",
        "assert df_result.shape[0] == df_test.shape[0] #check if prediction is same length as test\n",
        "df_result.to_csv('/content/drive/My Drive/Sentiment Analysis/result.csv')\n",
        "df_result.head(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xu4_-H8JiBzn",
        "colab_type": "text"
      },
      "source": [
        "Reference: https://colab.research.google.com/drive/19B73pxweO35DLT1Gju3tyuKmvATZFxIP?usp=sharing&fbclid=IwAR1QZPg8UWKv-4MBK9xSExUO93UOq2abolnFKOoGjOEhNgzp14fMAsTlW10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sBaN2vitn2o",
        "colab_type": "text"
      },
      "source": [
        "AutoModelForSequenceClassification:\n",
        "* DistilBertConfig \n",
        "* AlbertConfig \n",
        "* CamembertConfig \n",
        "* XLMRobertaConfig \n",
        "* BartConfig \n",
        "* LongformerConfig \n",
        "* RobertaConfig \n",
        "* BertConfig \n",
        "* XLNetConfig \n",
        "* MobileBertConfig \n",
        "* FlaubertConfig \n",
        "* XLMConfig \n",
        "* ElectraConfig."
      ]
    }
  ]
}